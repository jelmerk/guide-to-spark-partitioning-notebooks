{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.app.name" : "polynote",
        "spark.driver.memory" : "8g",
        "spark.default.parallelism" : "10",
        "spark.master" : "local[*]"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# Guide to spark partitioning: RDD Joins 1 / 2<br>\n",
        "\n",
        "\n",
        "This notebook hasÂ spark.default.parallelism set to 8\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354403970,
          "endTs" : 1604354404088
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.Partitioner\n",
        "import org.apache.spark.rdd.RDD"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354391437,
          "endTs" : 1604354391866
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.version"
      ],
      "outputs" : [
        {
          "execution_count" : 2,
          "data" : {
            "text/plain" : [
              "2.4.7"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "String"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354391975,
          "endTs" : 1604354392860
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sparkContext.getConf.getAll.foreach { case (key, value) => \n",
        "    println(s\"$key: $value\") \n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "spark.driver.host: 127.0.0.1\n",
            "spark.home: /home/jkuperus/Tools/spark-2.4.7-bin-hadoop2.6\n",
            "spark.app.id: local-1604354386984\n",
            "spark.executor.id: driver\n",
            "spark.driver.port: 37821\n",
            "spark.jars: /home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,/home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,/home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-utils/0.0.46/hnswlib-utils-0.0.46.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-scala_2.11/0.0.46/hnswlib-scala_2.11-0.0.46.jar,https://repo1.maven.org/maven2/org/eclipse/collections/eclipse-collections-api/9.2.0/eclipse-collections-api-9.2.0.jar,https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar,https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-spark_2.3.0_2.11/0.0.46/hnswlib-spark_2.3.0_2.11-0.0.46.jar,https://repo1.maven.org/maven2/org/eclipse/collections/eclipse-collections/9.2.0/eclipse-collections-9.2.0.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-core/0.0.46/hnswlib-core-0.0.46.jar\n",
            "spark.driver.extraJavaOptions: -Dlog4j.configuration=log4j.properties -Djava.library.path=/home/jkuperus/anaconda3/envs/polynote/lib/python3.7/site-packages/jep:/home/jkuperus/anaconda3/envs/polynote/lib\n",
            "spark.repl.class.outputDir: /tmp/spark-ff636457-7d52-4275-9580-41e9869d7eba/repl-b8181994-6c81-49d7-b5d4-011b3e8e7979\n",
            "spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
            "spark.repl.class.uri: spark://127.0.0.1:37821/classes\n",
            "spark.sql.catalogImplementation: hive\n",
            "spark.default.parallelism: 10\n",
            "spark.repl.local.jars: file:///home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,file:///home/jkuperus/dev/tools/polynote/deps/polynote-runtime.jar\n",
            "spark.driver.memory: 8g\n",
            "spark.master: local[*]\n",
            "spark.submit.deployMode: client\n",
            "spark.app.name: polynote\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354407446,
          "endTs" : 1604354407640
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "class CustomHashPartitioner(override val numPartitions: Int) extends Partitioner {\n",
        "  override def getPartition(key: Any): Int = key.hashCode % numPartitions\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354411248,
          "endTs" : 1604354411783
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedRddWithExplicitPartitioner(name: String,\n",
        "                                              numRecords: Int,\n",
        "                                              numPartitions: Int)\n",
        "        : RDD[(Int, String)] = {\n",
        "    val data = Range.inclusive(1, numRecords).map { value =>\n",
        "        value -> s\"$name-value\"\n",
        "    }\n",
        "    spark.sparkContext\n",
        "        .parallelize(data)\n",
        "        .partitionBy(new CustomHashPartitioner(numPartitions))\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354420158,
          "endTs" : 1604354420537
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedRdd(name: String,\n",
        "                       numRecords: Int,\n",
        "                       numPartitions: Int)\n",
        "        : RDD[(Int, String)] = {\n",
        "    val data = Range.inclusive(1, numRecords).map { value =>\n",
        "        value -> s\"$name-value\"\n",
        "    }\n",
        "    spark.sparkContext\n",
        "        .parallelize(data)\n",
        "        .repartition(numPartitions)\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 7,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## RDD's\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 8,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**1) **If no input Pair RDD (participating in the join operation) has a partitioner on the associated Key, then the number of partitions in the output joined RDD is equal to the value configured for the config property 'spark.default.parallelism'\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354424700,
          "endTs" : 1604354425764
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRdd(\"a\", 1000, 6)\n",
        "val b = createPartionedRdd(\"b\", 500, 2)\n",
        "\n",
        "a.join(b).getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 9,
          "data" : {
            "text/plain" : [
              "10"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 10,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**2)** If one or both input RDDs have a partitioner on the Key, then the maximum value of the number of partitions among the partitioner carrying input RDDs is compared with the config property 'spark.default.parallelism'\n",
        "\n",
        "**2a)** If the config property is set to some value and the maximum value is above the config value, then the maximum value is chosen as the number of partitions in the output joined RDD\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354431929,
          "endTs" : 1604354432516
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRddWithExplicitPartitioner(\"a\", 1000, 20)\n",
        "val b = createPartionedRdd(\"b\", 500, 2)\n",
        "\n",
        "println(a.join(b).getNumPartitions)\n",
        "println(a.join(b).partitioner == a.partitioner)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "20\n",
            "true\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 12,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**2b)**Â If the config property is set to some value and the maximum value is below that value, but, the maximum value is within a single order of magnitude of the config value, then the maximum value is chosen as the number of partitions in the output joined RDD\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354436063,
          "endTs" : 1604354436549
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRddWithExplicitPartitioner(\"a\", 1000, 6)\n",
        "val b = createPartionedRdd(\"b\", 500, 2)\n",
        "\n",
        "a.join(b).getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 13,
          "data" : {
            "text/plain" : [
              "6"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 14,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**2c)** If the config property is set to some value and the maximum value is below that value, but, the maximum value is not within a single order of magnitude of the highest number of partitions among input RDDs, then config property value is chosen as the number of partitions in the output joined RDD\n",
        "\n",
        "\n",
        "**This seems to be false and does not work on spark 2.3.2 and spark 2.4.7**<br>\n",
        "\n",
        "\n",
        "*Note in older versions of Spark, if either one or both parent Pair RDDs have partitioner on the Key object, then the maximum of number of partitions among partitioner contained input RDDs is always chosen as the number of partitions for the output joined RDD irrespective of any value being set for the config property 'spark.default.paralelism'*\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 15,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354440177,
          "endTs" : 1604354440636
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRddWithExplicitPartitioner(\"a\", 1000, 1)\n",
        "val b = createPartionedRddWithExplicitPartitioner(\"b\", 500, 1)\n",
        "\n",
        "a.join(b).getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 15,
          "data" : {
            "text/plain" : [
              "1"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 16,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
      ],
      "outputs" : [
      ]
    }
  ]
}