{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.app.name" : "polynote",
        "spark.driver.memory" : "8g",
        "spark.master" : "local[*]"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# Guide to spark partitioning: RDD Joins 2 / 2<br>\n",
        "\n",
        "\n",
        "This notebook has spark.default.parallelism unset\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354295433,
          "endTs" : 1604354295532
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.Partitioner\n",
        "import org.apache.spark.rdd.RDD"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354282414,
          "endTs" : 1604354282660
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.version"
      ],
      "outputs" : [
        {
          "execution_count" : 2,
          "data" : {
            "text/plain" : [
              "2.4.7"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "String"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354286973,
          "endTs" : 1604354287643
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "spark.sparkContext.getConf.getAll.foreach { case (key, value) => \n",
        "    println(s\"$key: $value\") \n",
        "}"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "spark.driver.port: 38599\n",
            "spark.driver.host: 127.0.0.1\n",
            "spark.repl.class.uri: spark://127.0.0.1:38599/classes\n",
            "spark.home: /home/jkuperus/Tools/spark-2.4.7-bin-hadoop2.6\n",
            "spark.executor.id: driver\n",
            "spark.jars: /home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,/home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,/home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-utils/0.0.46/hnswlib-utils-0.0.46.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-scala_2.11/0.0.46/hnswlib-scala_2.11-0.0.46.jar,https://repo1.maven.org/maven2/org/eclipse/collections/eclipse-collections-api/9.2.0/eclipse-collections-api-9.2.0.jar,https://repo1.maven.org/maven2/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar,https://repo1.maven.org/maven2/org/apache/spark/spark-avro_2.11/2.4.4/spark-avro_2.11-2.4.4.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-spark_2.3.0_2.11/0.0.46/hnswlib-spark_2.3.0_2.11-0.0.46.jar,https://repo1.maven.org/maven2/org/eclipse/collections/eclipse-collections/9.2.0/eclipse-collections-9.2.0.jar,https://repo1.maven.org/maven2/com/github/jelmerk/hnswlib-core/0.0.46/hnswlib-core-0.0.46.jar\n",
            "spark.driver.extraJavaOptions: -Dlog4j.configuration=log4j.properties -Djava.library.path=/home/jkuperus/anaconda3/envs/polynote/lib/python3.7/site-packages/jep:/home/jkuperus/anaconda3/envs/polynote/lib\n",
            "spark.serializer: org.apache.spark.serializer.KryoSerializer\n",
            "spark.app.id: local-1604354267940\n",
            "spark.sql.catalogImplementation: hive\n",
            "spark.repl.local.jars: file:///home/jkuperus/dev/tools/polynote/deps/polynote-spark-runtime.jar,file:///home/jkuperus/dev/tools/polynote/deps/polynote-runtime.jar\n",
            "spark.driver.memory: 8g\n",
            "spark.master: local[*]\n",
            "spark.submit.deployMode: client\n",
            "spark.app.name: polynote\n",
            "spark.repl.class.outputDir: /tmp/spark-535f2528-1298-4957-9efd-2fa902b16f7c/repl-02cc68a5-3088-424f-bf4f-5a8f0fbfdf8a\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354290288,
          "endTs" : 1604354290497
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "class CustomHashPartitioner(override val numPartitions: Int) extends Partitioner {\n",
        "  override def getPartition(key: Any): Int = key.hashCode % numPartitions\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354292442,
          "endTs" : 1604354293025
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedRddWithExplicitPartitioner(name: String,\n",
        "                                              numRecords: Int,\n",
        "                                              numPartitions: Int)\n",
        "        : RDD[(Int, String)] = {\n",
        "    val data = Range.inclusive(1, numRecords).map { value =>\n",
        "        value -> s\"$name-value\"\n",
        "    }\n",
        "    spark.sparkContext\n",
        "        .parallelize(data)\n",
        "        .partitionBy(new CustomHashPartitioner(numPartitions))\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354299254,
          "endTs" : 1604354299586
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedRdd(name: String,\n",
        "                       numRecords: Int,\n",
        "                       numPartitions: Int)\n",
        "        : RDD[(Int, String)] = {\n",
        "    val data = Range.inclusive(1, numRecords).map { value =>\n",
        "        value -> s\"$name-value\"\n",
        "    }\n",
        "    spark.sparkContext\n",
        "        .parallelize(data)\n",
        "        .repartition(numPartitions)\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 7,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "## RDD's\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 8,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**1)** If no input Pair RDD (participating in the join operation) has a partitioner on the associated Key, then the number of partitions in the output joined RDD is equal to the value configured for the config property 'spark.default.parallelism'\n",
        "\n",
        "\n",
        "However, if this config property is not set to any value, then the number of partitions in the output joined RDD is equal to the maximum of the number of partitions of input RDDS.<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354303865,
          "endTs" : 1604354304755
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRdd(\"a\", 1000, 6)\n",
        "val b = createPartionedRdd(\"b\", 500, 2)\n",
        "\n",
        "a.join(b).getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 9,
          "data" : {
            "text/plain" : [
              "6"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 10,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**2)** If one or both input RDDs have a partitioner on the Key, then the maximum value of the number of partitions among the partitioner carrying input RDDs is compared with the config property 'spark.default.parallelism'\n",
        "\n",
        "<br>**2d)** If the config property is not set, then the maximum value is chosen as the number of partitions in the output joined RDD.<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604354307193,
          "endTs" : 1604354307821
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRddWithExplicitPartitioner(\"a\", 1000, 6)\n",
        "val b = createPartionedRdd(\"b\", 500, 2)\n",
        "\n",
        "a.join(b).getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 11,
          "data" : {
            "text/plain" : [
              "6"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    }
  ]
}