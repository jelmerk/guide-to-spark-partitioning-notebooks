{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.app.name" : "polynote",
        "spark.driver.memory" : "4g",
        "spark.default.parallelism" : "8",
        "spark.master" : "local[*]"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# guide to spark partitioning: RDD aggregations 1\n",
        "\n",
        "\n",
        "<br class=\"Apple-interchange-newline\">This notebook has spark.default.parallelism set to 8<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604595997664,
          "endTs" : 1604595997738
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.HashPartitioner\n",
        "import org.apache.spark.rdd.RDD"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604596090914,
          "endTs" : 1604596091102
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedRddWithExplicitPartitioner(name: String,\n",
        "                                              numRecords: Int,\n",
        "                                              numPartitions: Int)\n",
        "        : RDD[(Int, String)] = {\n",
        "\n",
        "    val data = Range.inclusive(1, numRecords).map { value =>\n",
        "        value -> s\"$name-value\"\n",
        "    }\n",
        "    spark.sparkContext\n",
        "        .parallelize(data)\n",
        "        .partitionBy(new HashPartitioner(numPartitions))\n",
        "}\n",
        "\n",
        "def createPartionedRdd(name: String,\n",
        "                       numRecords: Int,\n",
        "                       numPartitions: Int)\n",
        "        : RDD[(Int, String)] = {\n",
        "    val data = Range.inclusive(1, numRecords).map { value =>\n",
        "        value -> s\"$name-value\"\n",
        "    }\n",
        "    spark.sparkContext\n",
        "        .parallelize(data)\n",
        "        .repartition(numPartitions)\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 2,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "If the input RDD has a partitioner on aggregation key, then the number of partitions in the aggregated output RDD is equal to the number of partitions in the input RDD.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604596073965,
          "endTs" : 1604596074295
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRddWithExplicitPartitioner(\"a\", 1000, 4)\n",
        "val b = a.reduceByKey((v1, v2) => v1)\n",
        "\n",
        "b.getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 3,
          "data" : {
            "text/plain" : [
              "4"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 5,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "If input RDD does not have a partitioner, then the number of partitions in the aggregated output RDD is equal to the value of ‘spark.default.parallelism’."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604596189020,
          "endTs" : 1604596189347
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRdd(\"a\", 1000, 4)\n",
        "val b = a.reduceByKey((v1, v2) => v1)\n",
        "\n",
        "b.getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 6,
          "data" : {
            "text/plain" : [
              "8"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 8,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "For all the three aggregation APIs, another flavor is also provided where the number of output partitions needs to be specified explicitly as the part of the\n",
        "APIs itself"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604596506731,
          "endTs" : 1604596507059
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedRdd(\"a\", 1000, 4)\n",
        "val b = a.reduceByKey((v1, v2) => v1, 99)\n",
        "\n",
        "b.getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 7,
          "data" : {
            "text/plain" : [
              "99"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    }
  ]
}