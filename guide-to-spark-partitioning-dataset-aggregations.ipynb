{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.app.name" : "polynote",
        "spark.driver.memory" : "4g",
        "spark.master" : "local[*]",
        "spark.sql.shuffle.partitions" : "50"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# guide to spark partitioning: DataSet aggregations\n",
        "\n",
        "\n",
        "This notebook has:\n",
        "\n",
        "spark.sql.shuffle.partitions set to 50\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604596894571,
          "endTs" : 1604596895508
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class MyRecord(key: Int, value: String)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604596975872,
          "endTs" : 1604596976577
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedDataset(name: String,\n",
        "                           numRecords: Int,\n",
        "                           numPartitions: Int)\n",
        "        : Dataset[MyRecord] = {\n",
        "    Range.inclusive(1, numRecords).map { value =>\n",
        "        MyRecord(value, s\"$name-value\")\n",
        "    }.toDS.repartition(numPartitions).localCheckpoint()\n",
        "}\n",
        "\n",
        "def createPartionedDatasetOnKey(name: String,\n",
        "                                numRecords: Int,\n",
        "                                numPartitions: Int)\n",
        "        : Dataset[MyRecord] = {\n",
        "    Range.inclusive(1, numRecords).map { value =>\n",
        "        MyRecord(value, s\"$name-value\")\n",
        "    }.toDS.repartition(numPartitions, $\"key\").localCheckpoint()\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 1,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "If the input Dataset (to be aggregated) is already partitioned strictly on the basis of either all or subset of the attributes of the aggregation key, then the output aggregated Dataset has the same number of partitions as in the parent Dataset. The input Dataset can already be partitioned due to a previous transformation of repartition, aggregation or join type.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599691854,
          "endTs" : 1604599692123
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDatasetOnKey(\"a\", 1000, 4)\n",
        "val b = a.groupBy($\"key\").count()\n",
        "\n",
        "println(a.queryExecution.executedPlan.outputPartitioning)\n",
        "\n",
        "b.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "hashpartitioning(key#370, 4)\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 2,
          "data" : {
            "text/plain" : [
              "4"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 5,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "If the input Dataset is not already partitioned on the basis of all or subset of the attributes of the aggregation key, then the number of partitions in the\n",
        "output aggregated Dataset is equal to value of Spark config ‘spark.sql.shuffle.partitions’, the default value for which is always set to 200."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599710641,
          "endTs" : 1604599710882
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDataset(\"a\", 1000, 4)\n",
        "val b = a.groupBy($\"key\").count()\n",
        "\n",
        "println(a.queryExecution.executedPlan.outputPartitioning)\n",
        "\n",
        "b.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "RoundRobinPartitioning(4)\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 6,
          "data" : {
            "text/plain" : [
              "50"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    }
  ]
}