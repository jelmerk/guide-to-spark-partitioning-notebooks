{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.sql.autoBroadcastJoinThreshold" : "0",
        "spark.app.name" : "polynote",
        "spark.driver.memory" : "8g",
        "spark.master" : "local[*]",
        "spark.sql.shuffle.partitions" : "50"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# guide to spark partitioning: DataSet joins 1\n",
        "\n",
        "\n",
        "This notebook has:\n",
        "\n",
        "\n",
        "spark.sql.shuffle.partitions set to 50\n",
        "\n",
        "\n",
        "spark.sql.autoBroadcastJoinThreshold set to 0\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599876171,
          "endTs" : 1604599877022
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class MyRecord(key: Int, value: String)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599880597,
          "endTs" : 1604599881897
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedDataset(name: String,\n",
        "                           numRecords: Int,\n",
        "                           numPartitions: Int)\n",
        "        : Dataset[MyRecord] = {\n",
        "    Range.inclusive(1, numRecords).map { value =>\n",
        "        MyRecord(value, s\"$name-value\")\n",
        "    }.toDS.repartition(numPartitions).localCheckpoint()\n",
        "}\n",
        "\n",
        "def createPartionedDatasetOnKey(name: String,\n",
        "                                numRecords: Int,\n",
        "                                numPartitions: Int)\n",
        "        : Dataset[MyRecord] = {\n",
        "    Range.inclusive(1, numRecords).map { value =>\n",
        "        MyRecord(value, s\"$name-value\")\n",
        "    }.toDS.repartition(numPartitions, $\"key\").localCheckpoint()\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 3,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "In Broadcast Hash join mechanism, one of the two input Datasets (participating in the Join) is broadcasted to all the executors.A Hash Table is being built on all the executors from the broadcasted Dataset, after which, each partition of the non-broadcasted input Dataset is joined independently to the other Dataset being available as a local hash table.\n",
        "\n",
        "It must be obvious from the functioning of Broadcast Hash Join that the number of output partitions of the resultant Dataset  is always equal to the number of partitions of the non broadcasted input Dataset.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599887863,
          "endTs" : 1604599894659
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDataset(\"a\", 1000, 4)\n",
        "val b = createPartionedDataset(\"b\", 500, 6)\n",
        "val c = a.join(broadcast(b), Seq(\"key\"))\n",
        "\n",
        "c.explain()\n",
        "\n",
        "c.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "== Physical Plan ==\n",
            "*(1) Project [key#2, value#3, value#12]\n",
            "+- *(1) BroadcastHashJoin [key#2], [key#11], Inner, BuildRight\n",
            "   :- Scan ExistingRDD[key#2,value#3]\n",
            "   +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)))\n",
            "      +- Scan ExistingRDD[key#11,value#12]\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 4,
          "data" : {
            "text/plain" : [
              "4"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 5,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Let us now see the rules deciding he output partitions of the Joined Dataset is decided for the Shuffle Hash and Sort Merge Join:\n",
        "\n",
        "**(1)** If none of the two input Datasets (participating in the Join transformation) is already hash partitioned on the respective Join Key(s), then the configured value of ‘spark.sql.shuffle.partitions’ is chosen as the number of partitions in the output joined\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599894723,
          "endTs" : 1604599895517
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDataset(\"a\", 1000, 4)\n",
        "val b = createPartionedDataset(\"b\", 1000, 6)\n",
        "val c = a.join(b, Seq(\"key\"))\n",
        "\n",
        "c.explain()\n",
        "\n",
        "c.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "== Physical Plan ==\n",
            "*(3) Project [key#30, value#31, value#40]\n",
            "+- *(3) SortMergeJoin [key#30], [key#39], Inner\n",
            "   :- *(1) Sort [key#30 ASC NULLS FIRST], false, 0\n",
            "   :  +- Exchange hashpartitioning(key#30, 50)\n",
            "   :     +- Scan ExistingRDD[key#30,value#31]\n",
            "   +- *(2) Sort [key#39 ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(key#39, 50)\n",
            "         +- Scan ExistingRDD[key#39,value#40]\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 6,
          "data" : {
            "text/plain" : [
              "50"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 7,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "**(2)** If one or both of the input Datasets are already hash partitioned based on respective Join Key(s), then the maximum value of\n",
        "the number of partitions among these input RDDs is compared against the configured value of ‘spark.sql.shuffle.partitions’ in the\n",
        "following ways to decide on the resultant number of partitions:\n",
        "\n",
        "\n",
        "**(2a)** If the maximum value is above the configured value of ‘spark.sql.shuffle.partitions’, then the maximum value is chosen as\n",
        "the number of partitions in the output joined Dataset.\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 8,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599895523,
          "endTs" : 1604599896579
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDatasetOnKey(\"a\", 1000, 123)\n",
        "val b = createPartionedDataset(\"b\", 1000, 6)\n",
        "val c = a.join(b, Seq(\"key\"))\n",
        "\n",
        "println(a.queryExecution.executedPlan.outputPartitioning)\n",
        "\n",
        "c.explain()\n",
        "\n",
        "c.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "hashpartitioning(key#57, 123)\n",
            "== Physical Plan ==\n",
            "*(3) Project [key#57, value#58, value#67]\n",
            "+- *(3) SortMergeJoin [key#57], [key#66], Inner\n",
            "   :- *(1) Sort [key#57 ASC NULLS FIRST], false, 0\n",
            "   :  +- Scan ExistingRDD[key#57,value#58]\n",
            "   +- *(2) Sort [key#66 ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(key#66, 123)\n",
            "         +- Scan ExistingRDD[key#66,value#67]\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 8,
          "data" : {
            "text/plain" : [
              "123"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 9,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "(2b) If the maximum value is below the configured value of ‘spark.sql.shuffle.partitions’, then the configured value is chosen as the number of partitions in the output joined Dataset. "
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599949006,
          "endTs" : 1604599949674
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDatasetOnKey(\"a\", 1000, 4)\n",
        "val b = createPartionedDataset(\"b\", 1000, 6)\n",
        "val c = a.join(b, Seq(\"key\"))\n",
        "\n",
        "println(a.queryExecution.executedPlan.outputPartitioning)\n",
        "\n",
        "c.explain()\n",
        "\n",
        "c.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "hashpartitioning(key#132, 4)\n",
            "== Physical Plan ==\n",
            "*(3) Project [key#132, value#133, value#142]\n",
            "+- *(3) SortMergeJoin [key#132], [key#141], Inner\n",
            "   :- *(1) Sort [key#132 ASC NULLS FIRST], false, 0\n",
            "   :  +- Exchange hashpartitioning(key#132, 50)\n",
            "   :     +- Scan ExistingRDD[key#132,value#133]\n",
            "   +- *(2) Sort [key#141 ASC NULLS FIRST], false, 0\n",
            "      +- Exchange hashpartitioning(key#141, 50)\n",
            "         +- Scan ExistingRDD[key#141,value#142]\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 10,
          "data" : {
            "text/plain" : [
              "50"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 11,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "In Cartesian Join, the number of partitions in the output Joined Dataset is always equal to product of number of partitions of the\n",
        "input Datasets"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604599905438,
          "endTs" : 1604599905836
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createPartionedDataset(\"a\", 1000, 5)\n",
        "val b = a.crossJoin(a)\n",
        "\n",
        "b.explain()\n",
        "\n",
        "b.rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "== Physical Plan ==\n",
            "CartesianProduct\n",
            ":- Scan ExistingRDD[key#111,value#112]\n",
            "+- Scan ExistingRDD[key#125,value#126]\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 13,
          "data" : {
            "text/plain" : [
              "25"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    }
  ]
}