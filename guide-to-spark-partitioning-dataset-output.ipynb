{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.master" : "local[*]",
        "spark.app.name" : "polynote",
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.driver.memory" : "4g"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# guide to spark partitioning: DataSet output\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650377998,
          "endTs" : 1604650378187
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import java.nio.file.Files\n",
        "import org.apache.spark.sql.SaveMode\n",
        "import sys.process._\n",
        "\n",
        "import scala.concurrent.forkjoin.ThreadLocalRandom"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650378191,
          "endTs" : 1604650379060
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val categories = Array(\"fruit\", \"cars\", \"animals\")\n",
        "\n",
        "case class MyRecord(key: Int, category: String, value: String)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650379071,
          "endTs" : 1604650380028
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createPartionedDataset(name: String,\n",
        "                           numRecords: Int,\n",
        "                           numPartitions: Int)\n",
        "        : Dataset[MyRecord] = {\n",
        "\n",
        "    Range.inclusive(1, numRecords).map { value =>\n",
        "\n",
        "        val randomCategory = categories(ThreadLocalRandom.current.nextInt(categories.size))\n",
        "        MyRecord(value, randomCategory, s\"$name-value\")\n",
        "    }.toDS.repartition(numPartitions).localCheckpoint()\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 4,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650380034,
          "endTs" : 1604650380193
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val tmpDir = Files.createTempDirectory(\"spark\").toFile.getCanonicalPath"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 5,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "The most basic approach that is adopted in Spark to write an RDD/Dataset on a filesystem is to write each partition of the\n",
        "RDD/Dataset in to a separate part file. All the part files, belonging to the same RDD/Dataset, are written in a common directory."
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 6,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650380198,
          "endTs" : 1604650385964
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "createPartionedDataset(\"a\", 1000, 4)\n",
        "    .write.mode(SaveMode.Overwrite).parquet(tmpDir)\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650385968,
          "endTs" : 1604650386249
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "s\"ls -l $tmpDir\".!"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "total 16\n",
            "-rw-r--r-- 1 jkuperus jkuperus 2009 Nov  6 09:13 part-00000-5ad010fb-a023-4e2a-8da6-fce7d2c7332d-c000.snappy.parquet\n",
            "-rw-r--r-- 1 jkuperus jkuperus 2009 Nov  6 09:13 part-00001-5ad010fb-a023-4e2a-8da6-fce7d2c7332d-c000.snappy.parquet\n",
            "-rw-r--r-- 1 jkuperus jkuperus 2009 Nov  6 09:13 part-00002-5ad010fb-a023-4e2a-8da6-fce7d2c7332d-c000.snappy.parquet\n",
            "-rw-r--r-- 1 jkuperus jkuperus 2043 Nov  6 09:13 part-00003-5ad010fb-a023-4e2a-8da6-fce7d2c7332d-c000.snappy.parquet\n",
            "-rw-r--r-- 1 jkuperus jkuperus    0 Nov  6 09:13 _SUCCESS\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 7,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 8,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "In this approach, which is only applicable to only Datasets, an additional ‘partitionBy’ expression is also specified in the writer API. Usually, the expression consists of one or more data fields of the Dataset schema. While writing the data records in a partition, for each of the records, firstly a sub-directory is identified based on the value of ‘partitionBy; expression evaluated for the record. The sub-directory has to be present within the primary directory being specified for the Dataset in the writer API. If the identified sub-directory is not present, then the same is created first. Eventually, the data record is written in the partition corresponding file within the identified sub-directory.\n",
        "\n",
        "‘partitionBy’ approach would prove helpful when you read back the written data by applying a filter on the basis of ‘partitionBy’ expression. Because, Spark can then only read the desired sub-directories according to filtering expression leaving the others\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 9,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650386255,
          "endTs" : 1604650386980
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "createPartionedDataset(\"a\", 1000, 4)\n",
        "    .write.mode(SaveMode.Overwrite).partitionBy(\"category\").parquet(tmpDir)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 10,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650387979,
          "endTs" : 1604650388238
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "s\"find $tmpDir -name *.parquet\".!"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "/tmp/spark6577489937337534733/category=animals/part-00003-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=animals/part-00002-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=animals/part-00001-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=animals/part-00000-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=fruit/part-00003-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=fruit/part-00002-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=fruit/part-00001-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=fruit/part-00000-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=cars/part-00003-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=cars/part-00002-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=cars/part-00001-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/category=cars/part-00000-6ae9ddb2-cfa8-4bdf-80b4-92bd61c9b199.c000.snappy.parquet\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 10,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 11,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650869140,
          "endTs" : 1604650869395
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val df = spark.read.parquet(tmpDir)\n",
        "\n",
        "println(df.rdd.getNumPartitions)\n",
        "println(df.queryExecution.executedPlan.outputPartitioning)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "6\n",
            "UnknownPartitioning(0)\n"
          ],
          "output_type" : "stream"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 12,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "In this approach, which is also applicable to only Datasets, an additional ‘bucketBy’ expression along with the number of buckets is also specified in the writer API. Usually, the expression consists of one or more data fields of the Dataset schema. Here, while writing the data records in a partition, for each record, a bucket is calculated first, based on which, the data record is written in a partition and bucket specific file. Therefore, if a partition can map all its data records to all the available buckets, then the number of files in the primary directory for that partition would be equal to the number of buckets. In the ‘bucketBy’ approach, if one also wants to store the bucketing specs then a table name is also additionally specified in the writer APIs. This table would then store the bucketing specs in the table meta space, which can be retrieved during the read operation .\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 13,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650395622,
          "endTs" : 1604650396921
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "createPartionedDataset(\"a\", 1000, 4)\n",
        "    .write.mode(SaveMode.Overwrite).option(\"path\", tmpDir).bucketBy(3, \"category\").saveAsTable(\"records\")"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 14,
      "metadata" : {
        "jupyter.source_hidden" : true,
        "cell.metadata.exec_info" : {
          "startTs" : 1604593867348,
          "endTs" : 1604593867459
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "s\"find $tmpDir -name *.parquet\".!"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "/tmp/spark6577489937337534733/part-00003-34cb36a6-04b0-419e-a67a-38da5b5f962a_00000.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00003-34cb36a6-04b0-419e-a67a-38da5b5f962a_00001.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00001-34cb36a6-04b0-419e-a67a-38da5b5f962a_00000.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00000-34cb36a6-04b0-419e-a67a-38da5b5f962a_00002.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00002-34cb36a6-04b0-419e-a67a-38da5b5f962a_00002.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00001-34cb36a6-04b0-419e-a67a-38da5b5f962a_00001.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00002-34cb36a6-04b0-419e-a67a-38da5b5f962a_00001.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00003-34cb36a6-04b0-419e-a67a-38da5b5f962a_00002.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00001-34cb36a6-04b0-419e-a67a-38da5b5f962a_00002.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00000-34cb36a6-04b0-419e-a67a-38da5b5f962a_00000.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00002-34cb36a6-04b0-419e-a67a-38da5b5f962a_00000.c000.snappy.parquet\n",
            "/tmp/spark6577489937337534733/part-00000-34cb36a6-04b0-419e-a67a-38da5b5f962a_00001.c000.snappy.parquet\n"
          ],
          "output_type" : "stream"
        },
        {
          "execution_count" : 14,
          "data" : {
            "text/plain" : [
              "0"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 15,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604650454262,
          "endTs" : 1604650454525
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val df = spark.read.table(\"records\")\n",
        "\n",
        "println(df.rdd.getNumPartitions)\n",
        "println(df.queryExecution.executedPlan.outputPartitioning)"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "3\n",
            "hashpartitioning(category#63, 3)\n"
          ],
          "output_type" : "stream"
        }
      ]
    }
  ]
}