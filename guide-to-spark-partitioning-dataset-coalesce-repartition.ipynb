{
  "metadata" : {
    "config" : {
      "dependencies" : {
        "scala" : [
          "org.apache.spark:spark-avro_2.11:2.4.4",
          "com.github.jelmerk:hnswlib-spark_2.3.0_2.11:0.0.46"
        ]
      },
      "exclusions" : [
        "com.google.guava:guava"
      ],
      "repositories" : [
        {
          "maven" : {
            "base" : "file:///home/user/.m2/repository"
          }
        }
      ],
      "sparkConfig" : {
        "spark.serializer" : "org.apache.spark.serializer.KryoSerializer",
        "spark.app.name" : "polynote",
        "spark.driver.memory" : "4g",
        "spark.master" : "local[*]",
        "spark.sql.shuffle.partitions" : "50"
      },
      "env" : {
        
      }
    },
    "language_info" : {
      "name" : "scala"
    }
  },
  "nbformat" : 4,
  "nbformat_minor" : 0,
  "cells" : [
    {
      "cell_type" : "markdown",
      "execution_count" : 0,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "# guide to spark partitioning: dataset coalesce / repartition\n",
        "\n",
        "spark.sql.shuffle.partitions is set to 50<br>\n",
        "\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 1,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604402159229,
          "endTs" : 1604402159379
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "import org.apache.spark.TaskContext"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 2,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604402147356,
          "endTs" : 1604402148258
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "case class MyRecord(key: Int, value: String)"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 3,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604402151035,
          "endTs" : 1604402152160
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "def createDataset(name: String,\n",
        "                  numRecords: Int,\n",
        "                  numPartitions: Int): Dataset[MyRecord] = {\n",
        "    Range.inclusive(1, numRecords).map { value =>\n",
        "        MyRecord(value, s\"$name-value\")\n",
        "    }.toDS\n",
        "}"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 6,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Repartitions the Dataset ‘A’ by the specified partitioning expressions using hash partitioning approach, the number of output partitions being implicitly specified by the config property ‘spark.sql.shuffle.partitions’\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 7,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604402235385,
          "endTs" : 1604402235647
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "createDataset(\"a\", 1000, 100).repartition($\"key\").rdd.getNumPartitions"
      ],
      "outputs" : [
        {
          "execution_count" : 7,
          "data" : {
            "text/plain" : [
              "50"
            ]
          },
          "metadata" : {
            "name" : "Out",
            "type" : "Int"
          },
          "output_type" : "execute_result"
        }
      ]
    },
    {
      "cell_type" : "markdown",
      "execution_count" : 4,
      "metadata" : {
        "language" : "text"
      },
      "language" : "text",
      "source" : [
        "Coalesce, being a narrow transformation, it shares the stage computation with the upstream transformations (on the upstream Datasets or RDDs) until a stage barrier is encountered in the upstream RDD DAG of the Spark Job. This sharing of the stage reduces the parallelism of the whole stage in proportion to the reduced number of partitions specified in the Coalesce API. Therefore, if you drastically reduce the number of partitions with coalesce, it would also drastically hit the parallelism of the transformations on the upstream Datasets or RDDs that share the same stage with coalesce.\n",
        "\n"
      ],
      "outputs" : [
      ]
    },
    {
      "cell_type" : "code",
      "execution_count" : 5,
      "metadata" : {
        "cell.metadata.exec_info" : {
          "startTs" : 1604402161980,
          "endTs" : 1604402167982
        },
        "language" : "scala"
      },
      "language" : "scala",
      "source" : [
        "val a = createDataset(\"a\", 1000, 100)\n",
        "\n",
        "a.map(_ => TaskContext.getPartitionId()).coalesce(2).distinct.show()"
      ],
      "outputs" : [
        {
          "name" : "stdout",
          "text" : [
            "+-----+\n",
            "|value|\n",
            "+-----+\n",
            "|    0|\n",
            "|    1|\n",
            "+-----+\n",
            "\n"
          ],
          "output_type" : "stream"
        }
      ]
    }
  ]
}